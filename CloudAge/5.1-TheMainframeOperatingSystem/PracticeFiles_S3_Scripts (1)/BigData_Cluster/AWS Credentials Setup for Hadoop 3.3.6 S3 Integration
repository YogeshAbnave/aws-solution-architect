# AWS Credentials Setup for Hadoop 3.3.6 S3 Integration

This document outlines different methods to configure AWS credentials for Hadoop 3.3.6 to access S3.

## Method 1: Configuration in core-site.xml (Simple but Less Secure)

Add the following properties to your `core-site.xml`:

```xml
<property>
    <name>fs.s3a.access.key</name>
    <value>YOUR_ACCESS_KEY</value>
</property>

<property>
    <name>fs.s3a.secret.key</name>
    <value>YOUR_SECRET_KEY</value>
</property>
```

## Method 2: Environment Variables (More Secure)

Set AWS credentials as environment variables:

```bash
export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY
export AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY
# Optional: Set region
export AWS_REGION=us-east-1
```

Add the following to your `core-site.xml` to use environment variables:

```xml
<property>
    <name>fs.s3a.aws.credentials.provider</name>
    <value>org.apache.hadoop.fs.s3a.EnvironmentVariableCredentialsProvider</value>
</property>
```

## Method 3: Hadoop Credential Providers (Most Secure)

### Step 1: Create a credential file

```bash
hadoop credential create fs.s3a.access.key -value YOUR_ACCESS_KEY -provider jceks://hdfs@nn.example.com:9001/user/root/awscreds.jceks
hadoop credential create fs.s3a.secret.key -value YOUR_SECRET_KEY -provider jceks://hdfs@nn.example.com:9001/user/root/awscreds.jceks
```

For a local file:
```bash
hadoop credential create fs.s3a.access.key -value YOUR_ACCESS_KEY -provider jceks://file/path/to/awscreds.jceks
hadoop credential create fs.s3a.secret.key -value YOUR_SECRET_KEY -provider jceks://file/path/to/awscreds.jceks
```

### Step 2: Configure Hadoop to use the credential provider

Add to `core-site.xml`:

```xml
<property>
    <name>hadoop.security.credential.provider.path</name>
    <value>jceks://file/path/to/awscreds.jceks</value>
</property>
```

## Method 4: IAM Roles (For EC2 Instances)

If running on EC2, use IAM roles for authentication:

```xml
<property>
    <name>fs.s3a.aws.credentials.provider</name>
    <value>org.apache.hadoop.fs.s3a.InstanceProfileCredentialsProvider</value>
</property>
```

## Method 5: AWS Profiles

If using AWS profiles:

```xml
<property>
    <name>fs.s3a.aws.credentials.provider</name>
    <value>org.apache.hadoop.fs.s3a.ProfileCredentialsProvider</value>
</property>

<property>
    <name>fs.s3a.profile</name>
    <value>default</value>
</property>
```

## Method 6: Multiple Authentication Providers

To try multiple authentication methods in sequence:

```xml
<property>
    <name>fs.s3a.aws.credentials.provider</name>
    <value>org.apache.hadoop.fs.s3a.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,org.apache.hadoop.fs.s3a.InstanceProfileCredentialsProvider</value>
</property>
```

## Testing Your Configuration

Test your S3 access with:

```bash
hadoop fs -ls s3a://your-bucket/
```

## Security Best Practices

1. **Never store AWS credentials in plain text** in configuration files that are checked into version control
2. **Use IAM roles** whenever possible for EC2 instances
3. **Create IAM users with minimal permissions** needed for your specific use case
4. **Regularly rotate credentials** for security
5. **Use Hadoop Credential Providers** to encrypt sensitive information
6. **Enable server-side encryption** for sensitive data in S3
7. **Audit S3 access** using AWS CloudTrail
