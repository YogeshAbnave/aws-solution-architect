# Complete Hadoop Cluster Setup Guide

This guide provides a **step-by-step setup** for configuring a **Hadoop Cluster with 6 machines** from scratch. Each section includes command explanations and configuration instructions to ensure beginners can follow along without confusion.

---

## ‚òÅÔ∏è Step 0: Create EC2 Cluster on AWS

Before starting the Hadoop setup, you need to launch and prepare your cloud infrastructure.

### üîß EC2 Configuration Steps:

1. **Log in to AWS Console**.

2. Navigate to **EC2 ‚Üí Instances ‚Üí Launch Instance**.

3. **Choose AMI**: Ubuntu Server 20.04 LTS (HVM), SSD Volume Type.

4. **Choose Instance Type**: t2.medium or t2.large (recommended for Hadoop).

5. **Configure Instances**:

   * Number of Instances: 6
   * Network: Default VPC
   * Auto-assign Public IP: Enable

6. **Add Storage**: Keep default or increase if needed.

7. **Add Tags**: For identification, use keys like `Name` and values like `nn`, `jt`, `dn1` etc.

8. **Configure Security Group**:

   * Allow SSH (port 22)
   * Allow Custom TCP (9000‚Äì9001 for Hadoop)
   * Allow All ICMP (for internal ping)
   * Add your IP in the source (or use 0.0.0.0/0 for testing only)

9. **Create or select key pair**: Save `.pem` file safely.

10. Click **Launch**.

Once the EC2 instances are up, proceed to the next section.

---

## üßæ Step 1: Launch and Name 6 EC2 Instances

Launch **6 Ubuntu EC2 instances** on AWS and name them as follows:

| Name | IP Address   | Role         |
| ---- | ------------ | ------------ |
| nn   | 172.31.0.208 | NameNode     |
| snn  | 172.31.1.86  | Secondary NN |
| jt   | 172.31.11.56 | Job Tracker  |
| dn1  | 172.31.11.76 | DataNode 1   |
| dn2  | 172.31.12.52 | DataNode 2   |
| dn3  | 172.31.12.81 | DataNode 3   |

## ‚úÖ Step 2: Connect All Machines Internally

* **SSH into each instance** using the private IP:

  ```bash
  ssh -i "key.pem" ubuntu@<Private-IP>
  ```

* Use the `arp` command to verify machines are connected on the internal network:

  ```bash
  arp
  ```

* Configure internal DNS using `/etc/hosts` on **each machine**:

  ```bash
  sudo nano /etc/hosts
  ```

  Add these lines:

  ```
  172.31.0.208 nn
  172.31.1.86 snn
  172.31.11.56 jt
  172.31.11.76 dn1
  172.31.12.52 dn2
  172.31.12.81 dn3
  ```

---

## üîê Step 3: Setup Passwordless SSH Login

1. **Copy your key to each machine**:

   ```bash
   scp -i Keyname.pem Keyname.pem ubuntu@<IP>:/home/ubuntu/.ssh/
   ```

2. **Set permissions and login**:

   ```bash
   chmod 400 Keyname.pem
   ssh -i Keyname.pem ubuntu@<machine>
   ```

3. **Setup SSH agent**:

   ```bash
   nano .profile
   ```

   Add:

   ```bash
   eval `ssh-agent`
   ssh-add /home/ubuntu/.ssh/Keyname.pem
   ```

   Then:

   ```bash
   source .profile
   ```

4. **Distribute `.profile` and key to all machines**:

   ```bash
   scp .profile snn:~/
   scp .ssh/Keyname.pem snn:/home/ubuntu/.ssh/
   ```

5. Disable SSH prompts for unknown hosts:

   ```bash
   sudo nano /etc/ssh/ssh_config
   ```

   Modify the line:

   ```
   StrictHostKeyChecking no
   ```

---

## üîÅ Step 4: Enable Two-Way SSH Connections Between All Machines

1. **Repeat the `/etc/hosts` entries on all nodes** with correct private IPs.
2. Set hostnames using:

   ```bash
   sudo hostname <hostname>
   ```

---

## üß© Step 5: DSH Setup (Distributed Shell)

DSH allows executing the same command across all nodes.

1. **Install DSH**:

   ```bash
   sudo apt install dsh
   ```
2. **Add all machine names**:

   ```bash
   sudo nano /etc/dsh/machines.list
   ```

   Add:

   ```
   nn
   snn
   jt
   dn1
   dn2
   dn3
   ```
3. **Test with**:

   ```bash
   dsh -a hostname
   dsh -a sudo apt update
   ```

---

## ‚òï Step 6: Install Java and Hadoop on All Machines

1. **Install Java**:

   ```bash
   dsh -a sudo apt install openjdk-8-jre -y
   dsh -a sudo apt install openjdk-8-jdk-headless
   ```
2. **Download and extract Hadoop**:

   ```bash
   dsh -a wget https://archive.apache.org/dist/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
   dsh -a tar -zxf hadoop-1.2.1.tar.gz
   dsh -a sudo mv hadoop-1.2.1 /usr/local/hadoop
   ```

---

## ‚öôÔ∏è Step 7: Configure Hadoop Files

### On `nn` and `jt` nodes:

Edit `.bashrc` and add:

```bash
export HADOOP_PREFIX=/usr/local/hadoop/
export PATH=$PATH:$HADOOP_PREFIX/bin
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME
```

Run:

```bash
source ~/.bashrc
```

Send to other node:

```bash
scp .bashrc jt:~/
```

### Edit Hadoop Configs:

Navigate:

```bash
cd /usr/local/hadoop/conf
```

#### Edit `hadoop-env.sh`:

```bash
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true
```

#### Edit `core-site.xml` (on nn):

```xml
<property>
  <name>fs.default.name</name>
  <value>hdfs://nn:9000</value>
</property>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/usr/local/hadoop/tmp</value>
</property>
```

#### Edit `mapred-site.xml` (on jt):

```xml
<property>
  <name>mapred.job.tracker</name>
  <value>hdfs://jt:9001</value>
</property>
```

#### Edit `slaves`:

```
dn1
dn2
dn3
```

#### Edit `masters`:

```
snn
```

---

## üöÄ Step 8: Distribute Hadoop Configuration Files

Send configs from nn:

```bash
scp hadoop-env.sh core-site.xml mapred-site.xml slaves masters snn:/usr/local/hadoop/conf/
scp hadoop-env.sh core-site.xml mapred-site.xml slaves masters jt:/usr/local/hadoop/conf/
scp hadoop-env.sh core-site.xml mapred-site.xml slaves masters dn1:/usr/local/hadoop/conf/
scp hadoop-env.sh core-site.xml mapred-site.xml slaves masters dn2:/usr/local/hadoop/conf/
scp hadoop-env.sh core-site.xml mapred-site.xml slaves masters dn3:/usr/local/hadoop/conf/
```

---

## üìÇ Step 9: Create TMP Directory

```bash
ssh nn
```

Then:

```bash
dsh -a mkdir /usr/local/hadoop/tmp
```

---

## üßº Step 10: Refresh Bash & Format Hadoop

Run:

```bash
dsh -a exec bash
```

Then:

```bash
hadoop namenode -format
```

---

## ‚ñ∂Ô∏è Step 11: Start Hadoop Services

### On Job Tracker:

```bash
ssh jt
start-mapred.sh
```

### On NameNode:

```bash
ssh nn
start-dfs.sh
```

---

## ‚úÖ Step 12: Verify Processes

Run:

```bash
dsh -a jps
```

Check output like:

* NameNode (nn)
* SecondaryNameNode (snn)
* JobTracker (jt)
* TaskTracker & DataNode (dn1, dn2, dn3)

You're done! üéâ

---

## ‚úÖ Summary

You have successfully:

* Launched a Hadoop EC2 cluster
* Set up 6 EC2 instances
* Enabled passwordless SSH
* Installed Java and Hadoop
* Configured Hadoop cluster
* Formatted NameNode
* Started the HDFS and MapReduce services
* Verified daemons using `jps`

You're now ready to run Hadoop jobs and scale your cluster further!



Here is the **complete Q\&A document with all options and detailed explanations** for all 40 questions:

---

### ‚úÖ **Linux & Shell Commands**

**Q1. Which command is used to check previously executed commands in Linux?**

* A) `top`
* B) `ps`
* C) `history` ‚úÖ
* D) `ls`
  **Explanation:** The `history` command displays the list of previously executed commands in the shell.

---

**Q2. What does the command `dsh -a sudo apt install openjdk-8-jre -y` do?**

* A) Installs Java locally
* B) Installs OpenJDK on all machines one by one ‚úÖ
* C) Installs Hadoop
* D) Updates all machines
  **Explanation:** `dsh -a` runs the command across all machines in the distributed shell group, and the rest installs OpenJDK.

---

**Q3. What does `!<command_number>` do in Linux?**

* A) Runs a script
* B) Deletes a command
* C) Executes a command from history by its number ‚úÖ
* D) Saves a command
  **Explanation:** The `!number` command re-executes a command from history based on its number.

---

### ‚úÖ **Hadoop Setup and Commands**

**Q4. What is the use of the `jps` command in a Hadoop setup?**

* A) Monitors disk space
* B) Lists running Java processes ‚úÖ
* C) Installs Java
* D) Shuts down servers
  **Explanation:** `jps` lists all Java processes, helping verify Hadoop daemons.

---

**Q5. What is the default hostname for macOS systems?**

* A) Linux
* B) UnixHost
* C) Darwin ‚úÖ
* D) MacHost
  **Explanation:** macOS is based on Darwin OS, hence the default kernel name is "Darwin".

---

**Q6. What is zsh?**

* A) A text editor
* B) A programming language
* C) A shell ‚úÖ
* D) A server
  **Explanation:** `zsh` (Z shell) is an enhanced shell with scripting features and auto-completion.

---

**Q7. Who is responsible for data governance?**

* A) Users
* B) Administrators ‚úÖ
* C) Developers
* D) Testers
  **Explanation:** Administrators ensure compliance, data access policies, and governance rules.

---

### ‚úÖ **Compliance & Industry Standards**

**Q8. What is the name of the telecom compliance authority in India?**

* A) HIPPA
* B) TRAI ‚úÖ
* C) FCC
* D) DOT
  **Explanation:** TRAI (Telecom Regulatory Authority of India) governs telecom policies and compliance.

---

**Q9. Which compliance is followed in the healthcare industry?**

* A) ISO
* B) GDPR
* C) HIPPA ‚úÖ
* D) TRAI
  **Explanation:** HIPAA (Health Insurance Portability and Accountability Act) is for healthcare data security in the US.

---

### ‚úÖ **AI & Machine Learning**

**Q10. Which of the following is a fundamental model?**

* A) OpenAI
* B) Cloud
* C) Entrophiam
* D) All of the above ‚úÖ
  **Explanation:** All entities develop or support foundational AI/ML models.

---

**Q11. What is the primary task of Machine Learning?**

* A) Coding
* B) Prediction ‚úÖ
* C) Storage
* D) Deployment
  **Explanation:** ML is used primarily to learn from data and make predictions or decisions.

---

**Q12. What is the main function of Generative AI?**

* A) Only predictions
* B) Data mining
* C) Generating new, relevant or non-relevant content ‚úÖ
* D) Logging
  **Explanation:** GenAI focuses on creating content like text, code, images, etc.

---

**Q13. Which AI platforms can be asked for social media‚Äìrelated insights?**

* A) Meta and Grok AI ‚úÖ
* B) OpenAI
* C) DeepSeek
* D) Gemini
  **Explanation:** Meta and Grok (from xAI) are optimized for social and conversational insights.

---

**Q14. Which AI is preferred for research purposes?**

* A) Gemini AI
* B) OpenAI
* C) Meta ‚ùå
* D) Grok
  **Explanation:** OpenAI is widely used in academia and research, while Meta was incorrectly selected here.

---

**Q15. Which AI is commonly used for analysis?**

* A) Gemini
* B) Meta
* C) OpenAI ‚úÖ
* D) DeepSeek
  **Explanation:** OpenAI's models like GPT-4 are used extensively for analytical tasks.

---

**Q16. Which of these is NOT an AI organization?**

* A) OpenAI
* B) DeepSeek
* C) Entrophiam
* D) Microsoft Word ‚úÖ
  **Explanation:** Microsoft Word is a product, not an AI company.

---

### ‚úÖ **Hadoop Configuration Files**

**Q17. What does the core-site.xml file configure in Hadoop?**

* A) Data nodes
* B) Resource manager
* C) Java environment
* D) NameNode settings ‚úÖ
  **Explanation:** It sets NameNode URI and basic filesystem settings.

---

**Q18. Which file contains settings for the resource manager in Hadoop?**

* A) hdfs-site.xml
* B) core-site.xml
* C) mapred-site.xml ‚úÖ
* D) yarn-site.xml
  **Explanation:** It includes job tracker and resource manager parameters.

---

**Q19. Which Hadoop configuration file relates to data nodes?**

* A) hdfs-site.xml ‚úÖ
* B) core-site.xml
* C) mapred-site.xml
* D) yarn-site.xml
  **Explanation:** It includes block size, replication factor, and DataNode settings.

---

**Q20. What is configured in the hadoop-env.sh file?**

* A) Storage paths
* B) Shell commands
* C) Java Home and environment variables ‚úÖ
* D) User authentication
  **Explanation:** It defines environment variables like JAVA\_HOME.

---

### ‚úÖ **Hadoop Commands and Execution**

**Q21. What does the command `hadoop namenode -format` do?**

* A) Stops the NameNode
* B) Formats the NameNode ‚úÖ
* C) Starts the DataNode
* D) Configures the job tracker
  **Explanation:** This prepares the HDFS file system from scratch.

---

**Q22. What does `start-dfs.sh` do in Hadoop?**

* A) Starts logical daemons
* B) Stops Hadoop services
* C) Starts physical daemons ‚úÖ
* D) Formats NameNode
  **Explanation:** It starts NameNode and DataNode processes.

---

**Q23. Which of the following are physical daemons started by `start-dfs.sh`?**

* A) TaskTracker
* B) JobTracker
* C) DataNode ‚úÖ
* D) ResourceManager
  **Explanation:** Physical daemons include DataNode and NameNode.

---

**Q24. How can you check physical daemons across machines in Hadoop?**

* A) jps
* B) `dsh -a jps` ‚úÖ
* C) ssh jps
* D) hdfs jps
  **Explanation:** `dsh -a` runs `jps` on all nodes, showing daemon status.

---

**Q25. Which command starts logical daemons in Hadoop?**

* A) start-logical.sh
* B) hadoop-start.sh
* C) start-mapred.sh ‚úÖ
* D) jps
  **Explanation:** It starts MapReduce components like JobTracker.

---

**Q26. Which are examples of logical daemons in Hadoop?**

* A) TaskTracker and JobTracker ‚úÖ
* B) DataNode and NameNode
* C) ResourceManager and NodeManager
* D) HDFS and YARN
  **Explanation:** Logical daemons relate to application layer tasks.

---

**Q27. What does the command `hadoop fs -ls /` do?**

* A) Lists Linux root
* B) Shows user folders
* C) Displays Hadoop root directory ‚úÖ
* D) Starts NameNode
  **Explanation:** It lists contents of the root directory in HDFS.

---

**Q28. What does the command `ls /` show?**

* A) Hadoop file system
* B) Linux root directory ‚úÖ
* C) File sizes
* D) Command history
  **Explanation:** This is a Linux command to list root (`/`) directory.

---

**Q29. What does `hadoop fs -cat /user/ubuntu/file` do?**

* A) Deletes a file
* B) Copies a file
* C) Displays the contents of a file ‚úÖ
* D) Shows permissions
  **Explanation:** `-cat` reads and displays file content from HDFS.

---

**Q30. What does `hadoop fs -ls /user/` display?**

* A) Permissions
* B) Cluster size
* C) Human users on the cluster ‚úÖ
* D) Admin logs
  **Explanation:** It shows user directories created in HDFS.

---

### ‚úÖ **Java & AWS Concepts**

**Q31. What is heap size associated with?**

* A) Hard disk
* B) Network settings
* C) Java JVM memory ‚úÖ
* D) RAM speed
  **Explanation:** JVM heap size defines the memory allocation for Java apps.

---

**Q32. What is it called when data is in the same Availability Zone (AZ)?**

* A) Replication
* B) Snapshot
* C) Copy ‚úÖ
* D) Mirroring
  **Explanation:** Copying within the same AZ doesn‚Äôt involve fault tolerance across zones.

---

**Q33. What is it called when data is stored in different Availability Zones (AZs)?**

* A) Copy
* B) Backup
* C) Replication ‚úÖ
* D) Snapshot
  **Explanation:** Replication across AZs ensures fault tolerance and high availability.

---

### ‚úÖ **Hadoop Example Jobs**

**Q34. What does `hadoop jar /usr/local/hadoop/hadoop-examples-1.2.1.jar pi 10 10000` do?**

* A) Runs a sorting job
* B) Installs Hadoop
* C) Runs the pi job ‚úÖ
* D) Starts DataNode
  **Explanation:** The Pi example estimates pi value using MapReduce.

---

**Q35. Which of the following is NOT part of `hadoop-examples-1.2.1.jar`?**

* A) wordcount
* B) terasort
* C) filecopy ‚úÖ
* D) pi
  **Explanation:** `filecopy` is not a default example in the standard examples JAR.

---

**Q36. Which Hadoop example job is used for calculating pi?**

* A) wordcount
* B) terasort
* C) pi ‚úÖ
* D) teravalidate
  **Explanation:** The Pi example uses Monte Carlo simulations.

---

**Q37. What is the purpose of `teravalidate` in Hadoop?**

* A) Validates TeraSort output ‚úÖ
* B) Runs unit tests
* C) Starts JobTracker
* D) Formats NameNode
  **Explanation:** `teravalidate` checks the correctness of the TeraSort result.

---

**Q38. Which of the following is a sorting program in Hadoop examples?**

* A) wordcount
* B) Option 2
* C) terasort ‚úÖ
* D) namenode
  **Explanation:** TeraSort is a benchmark tool for sorting massive datasets.

---

**Q39. What does `wordcount` do in the Hadoop examples?**

* A) Sorts numbers
* B) Counts words in input data ‚úÖ
* C) Finds prime numbers
* D) Formats disk
  **Explanation:** A classic example that counts word frequency in input files.

---

### ‚úÖ **AI Companies**

**Q40. Which AI company is NOT in the list of foundational models from your notes?**

* A) OpenAI
* B) Gemini
* C) Google  ‚úÖ
* D) Meta
  **Explanation:** Google is not listed in this context though they are developing LLaMA, but may not be part of the noted foundation models.

---

Would you like this in **Markdown or PDF format**?
